{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn import set_config\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data import load_data, convert_to_bool, convert_to_ordinal\n",
    "from src.feature_selection import FeatureImportance\n",
    "\n",
    "pd.set_option('display.precision',4)\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "plt.style.use('fivethirtyeight')\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the price of used cars\n",
    "In the data file {download}`ToyotaCorrolla <../data/ToyotaCorolla.csv>` we have data on used Toyotas from late 2004 in the Netherlands.  The goal is to predict the price based on it's specifications.\n",
    "\n",
    "Note there has been some significant EDA done ahead of time to reduce the number of columns and find interesting data.\n",
    "- Cylinders: dropped because all the values are 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's start by checking out the data\n",
    "cars_df = load_data('ToyotaCorolla',index_col='Id')\n",
    "cars_df = cars_df.drop(columns=['Cylinders','Model'])\n",
    "cars_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe it's worth noting (many notebooks leave these steps out because they are cumbersome and more complex to explain than to just do).  In a different notebook, I took a good look at the data to determine how to ensure that the data types were correct for each column.  You could do this in Excel (if your data is small enough) or you might use another tool where you can get a quick sample and try to understand the data.  For me, I took a look at the the types of data `cars_df.dtypes` then since I saw that many of the columns had just a 1 or a 0, I figured these were `boolean`.  Then looking at the names of the columns, like `Mfg_Year` and `Mfg_Month`, even though they are numbers, I felt it would be best to convert them what they are _ordinal_ values.  The other reason that this doesn't get much description is because the process is usually a bit non-linear and repeatitive.  For instance, once I determined the booleans, I converted them and then started looking for categoricals.  This would have taken 4 notebook cells (1. find bools, 2. convert columns to bool, 3. find non-bool number columns, 4. convert to ordinal).  As a reader you are really only interested in seeing the conversion of columns in a single cell instead of all the work behind it.\n",
    "\n",
    "Another example of the non-linear workflow, I discovered `Cylinders` as a unary value (all cars here have 4 cylinders).  If this column was dropped later in the notebook, then an update to the categorical columns as well.  Instead, I'll just let you know that I found this in a later step.  To be more clear, it was dropped immediately at the start before creating the categorical columns.  Additionally, I found later in the model that there are 300+ different car models in a dataset (within just 1400 rows), so this isn't going to be helpful later on either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = convert_to_bool(cars_df,['Met_Color', 'Automatic', 'Mfr_Guarantee', 'BOVAG_Guarantee', 'ABS',\n",
    "       'Airbag_1', 'Airbag_2', 'Airco', 'Automatic_airco', 'Boardcomputer',\n",
    "       'CD_Player', 'Central_Lock', 'Powered_Windows', 'Power_Steering',\n",
    "       'Radio', 'Mistlamps', 'Sport_Model', 'Backseat_Divider', 'Metallic_Rim',\n",
    "       'Radio_cassette', 'Parking_Assistant', 'Tow_Bar'])\n",
    "\n",
    "ord_columns = ['Model','Mfg_Month', 'Mfg_Year','Doors', 'Gears','Fuel_Type',\"Color\"]\n",
    "\n",
    "cars_df = convert_to_ordinal(cars_df,ord_columns)\n",
    "\n",
    "# Take out the target column before determining the number columns\n",
    "num_columns = cars_df.drop(columns='Price').select_dtypes(include='number').columns\n",
    "bool_columns = cars_df.select_dtypes(include='bool').columns\n",
    "cat_columns = cars_df.select_dtypes(include=['object','category']).columns\n",
    "\n",
    "print(num_columns)\n",
    "print(bool_columns)\n",
    "print(cat_columns)\n",
    "\n",
    "def remove_column(df, column_collection):\n",
    "       '''\n",
    "       Drop a column from the dataframe, as well as from the list associated with the column\n",
    "       '''\n",
    "       pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More EDA\n",
    "Next, I'm going to do a few more steps to see if I can determine anything interesting about the data.  I'll start with a cumulative density plot of the price.  This helps me to see what the range of prices is and I've also plotted to the IQR's, this can be helpful to see how spread out the prices are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iq1,iq2,iq3 = cars_df.Price.describe()[4:7]\n",
    "sns.kdeplot(cars_df.Price, cumulative=True);\n",
    "y=np.full(len(cars_df.Price.cumsum()),0.5)\n",
    "plt.axhline(y=0.5,color='red',linestyle='--');\n",
    "plt.axhline(y=0.75,color='green',linestyle='--');\n",
    "plt.axhline(y=0.25,color='green',linestyle='--');\n",
    "plt.text(x=0.1*max(cars_df.Price),y=0.51,s=f'Median: {iq2}');\n",
    "plt.text(x=0.1*max(cars_df.Price),y=0.76,s=f'IQ3: {iq3}');\n",
    "plt.text(x=0.1*max(cars_df.Price),y=0.26,s=f'IQ1: {iq1}');\n",
    "plt.show();\n",
    "\n",
    "cars_df.Price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, I want to see if  there are cases where there are too many of one class (bool or categorical)\n",
    "bool_df = cars_df[bool_columns].apply(pd.value_counts).T\n",
    "bool_df.columns = ['F','T']\n",
    "bool_df['% True'] = bool_df['T']/bool_df.sum(axis=1)\n",
    "bool_df.sort_values(by='% True')\n",
    "cars_df[cat_columns].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the values above, it seems like `Parking_Assist` is pretty much useless in terms of determining the price since there are only 4 cars with this feature - I'll remove it from the categoricals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see how these categoricals relate to price\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2,figsize=(20,25));\n",
    "for ax, c in zip(axs.flat,cat_columns[1:]):\n",
    "    sns.boxplot(data=cars_df, x=c,y='Price',ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see above that it looks like these values may have some interest (certainly the newer the car the more valuable)\n",
    "# There seems to be a small number of 2-doors let's see\n",
    "cars_df['Doors'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With only 2 samples of 2-door cars, we can safely drop these\n",
    "cars_df = cars_df.query('Doors>2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next I noticed that the price seems to differ for each color pretty significantly\n",
    "colors_df = cars_df.groupby('Color')['Price'].mean().sort_values(ascending=False)\n",
    "colors_df.columns = ['Color','MeanPrice']\n",
    "colors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "Many of the notebooks you see or some of the examples, skip the step where show how they determined that they only want to use 6-8 of the features that are provided.  I'm going to start by looking at which features are best using a DT regressor, a LinearModel with ForwardSelection and Backward elimination, once we've done this - we can start again by dropping out the features that don't seem to have any value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "X = cars_df.drop(columns='Price')\n",
    "y= cars_df['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size =0.2)\n",
    "cat_cols = ['Mfg_Month','Fuel_Type','Color']\n",
    "ord_cols = ['Mfg_Year','Doors', 'Gears']\n",
    "bool_cols = X_train.select_dtypes(include='bool').columns\n",
    "num_cols = X_train.select_dtypes(include=np.number).columns\n",
    "\n",
    "# We're going to setup our column transformer as pre-processing step for the follow-on steps.\n",
    "pre_processing = ColumnTransformer([\n",
    "    ('cat_encoder', OneHotEncoder(), cat_cols),\n",
    "    ('ord_encoder', OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=99), ord_cols),\n",
    "    ('num_encoder', StandardScaler(),num_cols)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"pp\",pre_processing), (\"dtr\",DecisionTreeRegressor(random_state=123))])\n",
    "\n",
    "pipe.fit(X_train,y_train)\n",
    "fi= FeatureImportance(pipe)\n",
    "fn = fi.get_feature_names()\n",
    "top_10_tree = fi.get_feature_importance().sort_values(ascending=False)[:10]\n",
    "top_10_tree.name='top_10_tree'\n",
    "top_10_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the `Age`, `KM`, `HP` and `Weight` seem to be the most important variables.  Let's take a look using a different method and see which features float to to the top.  Here we'll try a forest of trees, this ought to give us an even better idea of which parameters are of the most value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now one last try using GridSearch to find the best transformer\n",
    "param_grid = {'regressor__max_depth': [3,4,5]}\n",
    "\n",
    "pipe = Pipeline([(\"pp\",pre_processing), (\"regressor\",DecisionTreeRegressor(random_state=123))])\n",
    "reg = GridSearchCV(pipe,param_grid=param_grid)\n",
    "reg.fit(X_train, y_train)\n",
    "best= reg.best_estimator_\n",
    "\n",
    "fi= FeatureImportance(best)\n",
    "top_10_grid = fi.get_feature_importance().sort_values(ascending=False)[:10]\n",
    "top_10_grid.name='top_10_grid'\n",
    "top_10_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have both using a standard regressor and using GridSearch, let's compare\n",
    "pd.concat([top_10_grid,top_10_tree],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing what the top ten differentiating factors are help us to narrow the focus of our predictive models going forward.  Since we know for instance that a multiple linear regression model will continually improve as we add more variables (maybe insignificantly) and if we want to be able to explain our model, keeping to the most impactful variables most helpful.\n",
    "\n",
    "At this point, we could just decide to keep a managable number of factors and restart our entire process from scratch.  (Frankly this is what happens when alot of the samples are explained.  All the work that we've done here so far, is left out so that the learner is not subjected to the steps which simply eliminate useless information.)  Since all the work has been done, we are just going to move on keeping in all the features and letting the algorithm reduce the featureset as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "At this point we have done a thorough job of exploring the data, finding possible outliers, eliminating features and  coming up with a candidate list of good features.  We'll try a couple of different approaches finding a good predictive fit for the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cars = cars_df.drop(columns='Price')\n",
    "y_cars = cars_df['Price']\n",
    "\n",
    "c_cols = ['Mfg_Month','Fuel_Type','Color']\n",
    "o_cols = ['Mfg_Year','Doors', 'Gears']\n",
    "b_cols = X_train.select_dtypes(include='bool').columns\n",
    "n_cols = X_train.select_dtypes(include=np.number).columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cars,y_cars,test_size =0.2)\n",
    "\n",
    "# We're going to setup our column transformer as pre-processing step for the follow-on steps.\n",
    "pre_processing_transform = ColumnTransformer([\n",
    "    ('cat_encoder', OneHotEncoder(), c_cols),\n",
    "    ('ord_encoder', OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=99), o_cols),\n",
    "    ('num_encoder', StandardScaler(),n_cols)])\n",
    "\n",
    "X_train.columns.sort_values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit_transform(X_train[c_cols])\n",
    "\n",
    "ode = OrdinalEncoder()\n",
    "ode.fit_transform(X_train[o_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid2 = {'reg__max_depth': [3,4,5]}\n",
    "dtr = DecisionTreeRegressor(random_state=234)\n",
    "pipe2 = Pipeline([('pre',pre_processing_transform), (\"reg\",dtr)])\n",
    "dt_reg = GridSearchCV(pipe2,param_grid=param_grid2)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "best= dt_reg.best_estimator_\n",
    "best"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
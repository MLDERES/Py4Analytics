{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data import load_data\n",
    "from src.metric import classificationSummary\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    train_test_split, \n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.precision',4)\n",
    "plt.style.use('fivethirtyeight')\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling\n",
    "Ensembling refers to learning in which multiple models are used to produce a single outcome.  Often these models are considered \"weak learners\" because they intentionally try to underfit so that the final model is not overfit when the models are combined.\n",
    "\n",
    "There are generally two different types of ensembling:\n",
    "*   __Averaging__ - in these approaches multiple models are created and the outcomes are aggregated into a final result.  The aggregation made be via voting or by averaging the responses.\n",
    "    *  _Examples include_: Bagging methods, and Randomized Trees\n",
    "*   __Boosting__ - alternatively boosting methods take weaker models and run them in series.  This means that the output of each model provides a factor for subsequent models to use in the learning task.  \n",
    "    *  _Examples include_: AdaBoost, Gradient Tree Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German Credit\n",
    "# Assume all columns are categorical\n",
    "credit_df = load_data('GermanCredit', index_col=\"OBS#\",dtype='category')\n",
    "credit_df.rename(columns={\"RADIO/TV\":\"RADIO_TV\",\"CO-APPLICANT\":\"CO-APPLICANT\"}, inplace=True)\n",
    "\n",
    "credit_df = credit_df.astype({'DURATION':float,'AMOUNT':float, 'INSTALL_RATE':float, 'AGE':float,'NUM_CREDITS':float,'NUM_DEPENDENTS':float})\n",
    "credit_num_columns = credit_df.select_dtypes(include='number').columns\n",
    "credit_cat_columns = ['NEW_CAR', 'USED_CAR', 'FURNITURE', 'RADIO_TV', 'EDUCATION', 'RETRAINING', 'MALE_DIV', 'MALE_SINGLE', 'MALE_MAR_or_WID', 'CO-APPLICANT', 'GUARANTOR',  'REAL_ESTATE', 'PROP_UNKN_NONE', 'OTHER_INSTALL', 'RENT', 'OWN_RES', 'TELEPHONE', 'FOREIGN']\n",
    "credit_ord_columns = ['CHK_ACCT','HISTORY','SAV_ACCT','EMPLOYMENT','PRESENT_RESIDENT','JOB']\n",
    "credit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good for us we have a couple of ways to use Ensembling to make this work.  We can use the built in essemble methods like [BaggingClassifier](https://scikit-learn.org/stable/modules/ensemble.html#bagging).  These classifiers do all the heavy lifting for us, we specify the algorithm for the weak learner and let the BaggingClassifier manage the splitting and aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_credit = credit_df.drop(columns='RESPONSE')\n",
    "y_credit = credit_df.RESPONSE\n",
    "\n",
    "X_credit_train, X_credit_valid, y_credit_train, y_credit_valid = train_test_split(X_credit, y_credit, test_size=.40, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging with DecisionTrees\n",
    "clf_tree = DecisionTreeClassifier()\n",
    "clf_bag = BaggingClassifier(DecisionTreeClassifier(),n_estimators=10)\n",
    "\n",
    "clf_tree.fit(X_credit_train,y_credit_train)\n",
    "clf_bag.fit(X_credit_train,y_credit_train)\n",
    "\n",
    "y_credit_pred_valid_tree = clf_tree.predict(X_credit_valid)\n",
    "y_credit_pred_valid_bag = clf_bag.predict(X_credit_valid)\n",
    "\n",
    "print(f'Decision Tree Results')\n",
    "classificationSummary(y_credit_valid,y_credit_pred_valid_tree)\n",
    "print(f'Bag Results')\n",
    "classificationSummary(y_credit_valid,y_credit_pred_valid_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can use some of the Boosting methods like [AdaBoost](https://scikit-learn.org/stable/modules/ensemble.html#adaboost) or [GradientTreeBoosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting).  While both are good in their results, one of more popular alternatives which is usually at the top of many Kaggle competitions is [XGBoost](https://xgboost.readthedocs.io/en/stable/install.html#python).  There are a few things to keep in mind when using XGBoost though:\n",
    "- Categorical fields must be encoded\n",
    "- Numeric features need to be standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "pre_processor = ColumnTransformer(transformers=[\n",
    "    ('numeric',StandardScaler(),credit_num_columns),\n",
    "    ('categories',OneHotEncoder(),credit_cat_columns),\n",
    "    ('ordinals',OneHotEncoder(),credit_ord_columns)])\n",
    "\n",
    "X_credit_processed = pre_processor.fit_transform(X_credit)\n",
    "y_credit_processed = LabelEncoder().fit_transform(y_credit)\n",
    "\n",
    "X_credit_xgb_train, X_credit_xgb_valid, y_credit_xgb_train, y_credit_xgb_valid = train_test_split(X_credit_processed, y_credit_processed, train_size=.40, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb = xgb.XGBClassifier(use_label_encoder=False,eval_metric='auc',objective='binary:logistic',verbosity=0)\n",
    "clf_xgb.fit(X_credit_xgb_train,y_credit_xgb_train)\n",
    "y_credit_pred_valid_xgb = clf_xgb.predict(X_credit_xgb_valid)\n",
    "\n",
    "print(f'XGBoost Results')\n",
    "classificationSummary(y_credit_xgb_valid,y_credit_pred_valid_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is going to take a long time to run, so pick your parameters wisely!\n",
    "num_splits = 5\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 4, 5, 7],\n",
    "    \"learning_rate\": [0.1, 0.01, 0.05],\n",
    "    #\"gamma\": [0, 0.25, 1],\n",
    "    # \"reg_lambda\": [0, 1, 10],\n",
    "    # \"scale_pos_weight\": [1, 3, 5],\n",
    "    \"subsample\": [0.5],\n",
    "    # \"colsample_bytree\": [0.5],\n",
    "}\n",
    "# Go through an exhaustive search by combining all the variables specified in the\n",
    "clf_xgb_2 = xgb.XGBClassifier(use_label_encoder=False,eval_metric='auc',objective='binary:logistic',verbosity=0)\n",
    "clf_xgb_grid = GridSearchCV(clf_xgb_2, cv=num_splits, param_grid=param_grid, n_jobs=-1)\n",
    "clf_xgb_grid.fit(X_credit_xgb_train,y_credit_xgb_train)\n",
    "y_credit_pred_valid_xgb_grid = clf_xgb.predict(X_credit_xgb_valid)\n",
    "\n",
    "print(f'XGBoost Results')\n",
    "classificationSummary(y_credit_xgb_valid,y_credit_pred_valid_xgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_estimator = clf_xgb_grid.best_estimator_\n",
    "best_max_depth = xgb_best_estimator.get_params()[\"max_depth\"]\n",
    "best_learning_rate = xgb_best_estimator.get_params()[\"learning_rate\"]\n",
    "print(f'{best_max_depth=}, {best_learning_rate=}')\n",
    "\n",
    "# We could also then use the best_estimator to deploy\n",
    "xgb_best_estimator.predict(X_credit_xgb_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='center'/>\n",
    "\n",
    "[Back to TOC](./00-Introduction.ipynb)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
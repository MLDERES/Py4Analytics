{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differing data formats\n",
    "In this notebook, we'll look at a few different data formats that are common in analytics.  This includes, delimited formats (.csv), extensible markup language (.xml) and javascript notation (.json) formats.  There are several other types of data files that you may come across, but these are common for a couple reasons.\n",
    "* Portability.  This means that the files can be shared across different kinds of computer operating systems without too much trouble\n",
    "* Readibility.  There is no special encoding for these files.  They can be created or opened with a standard text editor.  \n",
    "* Compatibility.  Because they are easy to pass around and simple to understand they are supported by lots and lots of platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delimited formats\n",
    "Probably the most common format for portable files is the delimited format (often comma-delimited).  In these files, each line represents a single record and the fields of each record are denoted by some kind of special separator character (usually a comma and sometimes a tab or space).  There are no rules for these kinds of files, just conventions.  For instance, the first line of the file is typically a \"header\" record.  This record serves to describe the contents in the rows that follow.  This makes it straightforward for someone reading the file to understand what is expected in the data rows.  Take for instance\n",
    "> \n",
    "> ```\n",
    "> Id,Name,Phone Number\n",
    "> 1,Alice,555-1234\n",
    "> 2,Bob,555-0898\n",
    "> 3,Charlie,\n",
    "> 4,Doug,867-5309\n",
    "> ```\n",
    "\n",
    "In this example, we can see clearly that there are 4 records with the id values from 1 to 4.  The first record, that is with id 1, has a name of Alice and a Phone Number of 555-1234.  While we can easily see each of the other records- we can tell in this simple example that line 4 is missing a phone number.\n",
    "\n",
    "While easy to read for simple/small files - it becomes increasingly complex to read this file in a text editor if we are to try and find errors and missing values.  Fortunately, we can use the tools we have to import the file rather easily and find missing values, misaligned fields and generally interpret the data.\n",
    "\n",
    "### Reading/Writting Delimited Files\n",
    "\n",
    "There are essentially two operations required to read a text file using Python\n",
    "1.  Create a connection to the file we want to read or write.\n",
    "2.  Read each line of the file, one-by-one.\n",
    "3.  (Optionally) Do something which each line as we read it.\n",
    "\n",
    "The operating system (Windows, Linux, MacOS) is responsible for ensuring that files are safely stored and accessed (it would be awful if a process deleted a file we were trying to read, or if it were to write to a file at the same time our program was trying to do so!).  Therefore, Python asks for what is called a stream.  You can think of a stream like an electronic key to a hotel room, when you check into a hotel you get a key to a room and when you leave you checkout and your key no longer works.  A stream gives Python permission to read or make changes to the file in question and when we are done, we need to give back the key.\n",
    "\n",
    "An example of reading a text file follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarter,Shipments\n",
      "Q1-1985,4009\n",
      "Q2-1985,4321\n",
      "Q3-1985,4224\n",
      "Q4-1985,3944\n",
      "Q1-1986,4123\n",
      "Q2-1986,4522\n",
      "Q3-1986,4657\n",
      "Q4-1986,4030\n",
      "Q1-1987,4493\n",
      "Q2-1987,4806\n",
      "Q3-1987,4551\n",
      "Q4-1987,4485\n",
      "Q1-1988,4595\n",
      "Q2-1988,4799\n",
      "Q3-1988,4417\n",
      "Q4-1988,4258\n",
      "Q1-1989,4245\n",
      "Q2-1989,4900\n",
      "Q3-1989,4585\n",
      "Q4-1989,4533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# open a file in read mode\n",
    "appliance_filestream = open('../data/ApplianceShipments.csv', 'r')\n",
    "# read the file\n",
    "data = appliance_filestream.read()\n",
    "# close the file\n",
    "appliance_filestream.close()\n",
    "# print the data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common pattern is to open the file (for reading), process the file line by line and then close the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarter,Shipments\n",
      "\n",
      "Q1-1985,4009\n",
      "\n",
      "Q2-1985,4321\n",
      "\n",
      "Q3-1985,4224\n",
      "\n",
      "Q4-1985,3944\n",
      "\n",
      "Q1-1986,4123\n",
      "\n",
      "Q2-1986,4522\n",
      "\n",
      "Q3-1986,4657\n",
      "\n",
      "Q4-1986,4030\n",
      "\n",
      "Q1-1987,4493\n",
      "\n",
      "Q2-1987,4806\n",
      "\n",
      "Q3-1987,4551\n",
      "\n",
      "Q4-1987,4485\n",
      "\n",
      "Q1-1988,4595\n",
      "\n",
      "Q2-1988,4799\n",
      "\n",
      "Q3-1988,4417\n",
      "\n",
      "Q4-1988,4258\n",
      "\n",
      "Q1-1989,4245\n",
      "\n",
      "Q2-1989,4900\n",
      "\n",
      "Q3-1989,4585\n",
      "\n",
      "Q4-1989,4533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# open a file in read mode\n",
    "appliance_filestream = open('../data/ApplianceShipments.csv', 'r')\n",
    "# read one line at a time\n",
    "for line in appliance_filestream.readlines():\n",
    "    print(line)\n",
    "# close the file\n",
    "appliance_filestream.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `with` blocks\n",
    "What happens if we forget to close the file?  If we don't close the file, Python keeps it open for us and no one else (or any other process) will be able to access the file until our program exits.  For simple programs this isn't usually a problem, but it can lead to unexpected results if our program is large.  What's worse is that Python doesn't remind us to the close the file with any kind of special error.  Fortunately, what Python does provide is a special block called `with`.  Similar to function blocks, or if-else blocks we have seen earlier, `with` blocks have a scope that is limited to the lines which are indented in the block.  A `with` block creates a file stream for us, assigns it to the variable we assign, and when the block is complete - closes the file stream automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarter,Shipments\n",
      "\n",
      "Q1-1985,4009\n",
      "\n",
      "Q2-1985,4321\n",
      "\n",
      "Q3-1985,4224\n",
      "\n",
      "Q4-1985,3944\n",
      "\n",
      "Q1-1986,4123\n",
      "\n",
      "Q2-1986,4522\n",
      "\n",
      "Q3-1986,4657\n",
      "\n",
      "Q4-1986,4030\n",
      "\n",
      "Q1-1987,4493\n",
      "\n",
      "Q2-1987,4806\n",
      "\n",
      "Q3-1987,4551\n",
      "\n",
      "Q4-1987,4485\n",
      "\n",
      "Q1-1988,4595\n",
      "\n",
      "Q2-1988,4799\n",
      "\n",
      "Q3-1988,4417\n",
      "\n",
      "Q4-1988,4258\n",
      "\n",
      "Q1-1989,4245\n",
      "\n",
      "Q2-1989,4900\n",
      "\n",
      "Q3-1989,4585\n",
      "\n",
      "Q4-1989,4533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# open a file in read mode and set the filestream name to appliance_filestream\n",
    "with open('../data/ApplianceShipments.csv', mode='r') as appliance_filestream:\n",
    "    \n",
    "    for line in appliance_filestream.readlines():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leveraging specialized libraries\n",
    "This works great if we want to deal with generic text files.  Each line we read is returned from the `readlines()` function as a string.  We can do whatever can be done with strings.  We can `split` the string into parts and assign each part to a variable or just print one part.  Let's read a string and add up the sales from each line in the file.  Notice that we are reading the first line outside of the loop because it doesn't have a number in the second spot.  If we don't read the first line before we loop, we'll end up trying to convert the word 'shipment' into a number which won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88497.0\n"
     ]
    }
   ],
   "source": [
    "# initialize a variable to hold the total\n",
    "total_sales = 0\n",
    "\n",
    "# open a file in read mode and set the filestream name to appliance_filestream\n",
    "with open('../data/ApplianceShipments.csv', 'r') as appliance_filestream:\n",
    "    # read the header line and discard it\n",
    "    header = appliance_filestream.readline()\n",
    "\n",
    "    # read each of the next lines in the file\n",
    "    for line in appliance_filestream.readlines():\n",
    "        # split the line into a list\n",
    "        line_list = line.split(',')\n",
    "        # get the sales amount\n",
    "        sales_amount = line_list[1]\n",
    "        # add the sales amount to the total\n",
    "        total_sales = total_sales + float(sales_amount)\n",
    "print (total_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the great things about Python is that there are so many people using it that common operations are often already figured out and saved for us in libraries to make it much simpler for us.  The `csv` module is one such library.  Using the `csv` module, we can read each line in the as an array (or dictionary) without all the extra stuff we did above.  Additionally, if we are using some other separator (like tab or space) we can specify this as part of the `reader()` initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'Y\\x17\\xd8=~\\xc0\\xc2|\\xddwHx\\xfc\\x03\\xba\\xb0\\xa4\\x07 \\xb6\\xd7\\xcfU3\\xccY5\\x15B\\xf7\\xb3\\x8c\\x84\\x8f\\x17\\xe4\\x9b\\x15%\\x8bs\\xc1\\xf2\\xd6\\xe0\\x10U\\xdd\\xb91\\x1e\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08']\n",
      "Bad pipe message: %s [b'\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 Zz9e\\xd8\\x7f\\x951a\\nN\\xbf\\xd2\\x8f\\x1b\\x06o\\x98\\x9f\\xfc\\x0fn']\n",
      "Bad pipe message: %s [b'\\xb4\\xbai\\xc2\\xde\\x8a\\xb9k,\\xf5\\xc6\\xc8\\x9f#\\x94\\x9c\\x0f] \\xd4i\\x81\\xb3\\xa8\\xa2\\x15P\\xc1\\x15\\x8ev\\x0b\"/\\x9f\\xafx\\xa6$\\xed\\x93(\\x19~\\x0e\\xc2\\xc0\\xd1\\')\\xca\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\x1e\\x87\\xdc\\xd1\\x9b\\x0f\\x1a=U\\xa8\\xf3S\\xea\\xc4\\xde0I\\xe2']\n",
      "Bad pipe message: %s [b'W\\xecS\\xb3\\x82$\\xf2\\xcd\\x9d\\x9bb\\\\\\xb3\\xca\\xfbbW*\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0', b\"$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\"]\n",
      "Bad pipe message: %s [b\"\\x92ws\\xe2\\x1aCi\\xd0\\\\\\x93\\xef\\xbd\\xa89c'\\x86\\x00\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\"]\n",
      "Bad pipe message: %s [b\"\\xb5S#\\x9b\\xa0'\\xf0$\\x98\\xfaOv^`\\xe5\\x84f0\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\"]\n",
      "Bad pipe message: %s [b'\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x03\\xc0\\x10']\n",
      "Bad pipe message: %s [b'\\xfb\\x10A^\\xde\\xe0\\x92\\xd0p;\\xb4A\\\\\\xaa\\xb5\\x94gs\\x00\\x00\\xf4\\xc00\\xc0', b'(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7']\n"
     ]
    }
   ],
   "source": [
    "# Import the csv module \n",
    "import csv\n",
    "# reset the total_sales variable\n",
    "total_sales = 0\n",
    "\n",
    "# open a file in read mode and set the filestream name to appliance_filestream\n",
    "with open('../data/ApplianceShipments.csv', 'r') as appliance_filestream:\n",
    "    # read the file using the csv module\n",
    "    appliance_reader = csv.reader(appliance_filestream, delimiter=',')\n",
    "    # read the header line and discard it\n",
    "    header = next(appliance_reader)\n",
    "    # read each of the next lines in the file\n",
    "    for line in appliance_reader:\n",
    "        # get the sales amount\n",
    "        sales_amount = line[1]\n",
    "        # add the sales amount to the total\n",
    "        total_sales = total_sales + float(sales_amount)\n",
    "print (total_sales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `pandas` library\n",
    "\n",
    "We can certainly read data files line by line and process them this way, but most often, for our purposes we are looking to do something specific like evaluate the data, use it for analysis or convert it to another format.  If you are interested in the nuts and bolts of reading files line by line then I would refer you to [this documentation](https://docs.python.org/3/library/csv.html) or simply google 'reading csv file in Python'.  In order to use the data effectively, we'll depend instead on our ever useful and super-handy `pandas` library.\n",
    "\n",
    "The `pandas` library actually can help us to read lots of different file formats from CSV files and Excel files, XML, JSON and even web pages (and suprisingly the clipboard!).  Mostly they work a lot alike, so we'll focus on the format and the nuances of the most common cases you'll run across.\n",
    "\n",
    "````{admonition} Optional Parameters\n",
    "The `read_csv()` function takes a number of optional parameters, so it's best to be explicit about what you mean rather than calling the function and counting on the order of the parameters.  Recall that in a function definition if the parameter is defined with an `=` after it, this means the parameter is optional and if not specified will use the default value as specified in the parameter definition.  Follow this link for a [refresher on optional parameters](../extras/101-Functions.ipynb#optional-parameters).\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only specifying `a`\n",
      "Hello 0 all\n",
      "Providing `a` and `b` only\n",
      "Hello 12 all\n",
      "Providing `a` and `b` explicitly\n",
      "Hello 12 all\n",
      "Providing `a` (by position) and `c` explicitly.\n",
      "Hello 0 everyone\n"
     ]
    }
   ],
   "source": [
    "def my_func(a, b=0, c='all'):\n",
    "    print(a, b, c)\n",
    "\n",
    "print(\"Only specifying `a`\")\n",
    "my_func('Hello')\n",
    "print(\"Providing `a` and `b` only\")\n",
    "my_func('Hello', 12)\n",
    "print(\"Providing `a` and `b` explicitly\")\n",
    "my_func(a='Hello',b=12)\n",
    "print(\"Providing `a` (by position) and `c` explicitly.\")\n",
    "my_func('Hello',c='everyone')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv#pandas.read_csv) function.  The function definition can be intimidating because it has lots of options, but this is also helpful to ensure that the operation works exactly as we expect it to.  For instance, there are parameters which define what the delimiter is (especially if we choose not to use commas), an option to specify the field names, several parameters which help specify the date format (day first) and whether the file has an index defined in it.\n",
    "\n",
    "In each case, we are interested in reading the file into a dataframe and then working on it with the tools we know about dataframes.  Let's look at a couple of simple examples.  All the data files can be found in the `data` directory so you can take a look in any text editor to read the file.\n",
    "{download}`ApplianceShipments.csv <../data/ApplianceShipments.csv>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Shipments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1-1985</td>\n",
       "      <td>4009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2-1985</td>\n",
       "      <td>4321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q3-1985</td>\n",
       "      <td>4224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q4-1985</td>\n",
       "      <td>3944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q1-1986</td>\n",
       "      <td>4123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q2-1986</td>\n",
       "      <td>4522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q3-1986</td>\n",
       "      <td>4657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q4-1986</td>\n",
       "      <td>4030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q1-1987</td>\n",
       "      <td>4493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q2-1987</td>\n",
       "      <td>4806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Q3-1987</td>\n",
       "      <td>4551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Q4-1987</td>\n",
       "      <td>4485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Q1-1988</td>\n",
       "      <td>4595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Q2-1988</td>\n",
       "      <td>4799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Q3-1988</td>\n",
       "      <td>4417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Q4-1988</td>\n",
       "      <td>4258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Q1-1989</td>\n",
       "      <td>4245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Q2-1989</td>\n",
       "      <td>4900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Q3-1989</td>\n",
       "      <td>4585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Q4-1989</td>\n",
       "      <td>4533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Quarter  Shipments\n",
       "0   Q1-1985       4009\n",
       "1   Q2-1985       4321\n",
       "2   Q3-1985       4224\n",
       "3   Q4-1985       3944\n",
       "4   Q1-1986       4123\n",
       "5   Q2-1986       4522\n",
       "6   Q3-1986       4657\n",
       "7   Q4-1986       4030\n",
       "8   Q1-1987       4493\n",
       "9   Q2-1987       4806\n",
       "10  Q3-1987       4551\n",
       "11  Q4-1987       4485\n",
       "12  Q1-1988       4595\n",
       "13  Q2-1988       4799\n",
       "14  Q3-1988       4417\n",
       "15  Q4-1988       4258\n",
       "16  Q1-1989       4245\n",
       "17  Q2-1989       4900\n",
       "18  Q3-1989       4585\n",
       "19  Q4-1989       4533"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read a file from the data directory\n",
    "shipments_df = pd.read_csv('../data/ApplianceShipments.csv')\n",
    "shipments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the files have an identifier that we want to keep.  For instance the bankruptcy file has an account number which we want to use as our index.  We can tell pandas to keep this as the index rather than specifying a new one.  Here we are using the \n",
    "{download}`Bankruptcy.csv <../data/Bankruptcy.csv>` file found in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D</th>\n",
       "      <th>YR</th>\n",
       "      <th>R1</th>\n",
       "      <th>R2</th>\n",
       "      <th>R3</th>\n",
       "      <th>R4</th>\n",
       "      <th>R5</th>\n",
       "      <th>R6</th>\n",
       "      <th>R7</th>\n",
       "      <th>R8</th>\n",
       "      <th>...</th>\n",
       "      <th>R15</th>\n",
       "      <th>R16</th>\n",
       "      <th>R17</th>\n",
       "      <th>R18</th>\n",
       "      <th>R19</th>\n",
       "      <th>R20</th>\n",
       "      <th>R21</th>\n",
       "      <th>R22</th>\n",
       "      <th>R23</th>\n",
       "      <th>R24</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NO</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.19</td>\n",
       "      <td>10.36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.23</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.22</td>\n",
       "      <td>3.78</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.41</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>13.29</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.55</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.36</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.12</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>7.74</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    D  YR    R1    R2    R3    R4    R5    R6    R7     R8  ...   R15   R16  \\\n",
       "NO                                                          ...               \n",
       "1   0  78  0.23  0.08  0.02  0.03  0.46  0.12  0.19  10.36  ...  0.05  0.57   \n",
       "2   0  77  0.19  0.07  0.09  0.12  0.02  0.02  0.03   3.13  ...  0.09  0.12   \n",
       "3   0  72  0.07  0.02  0.03  0.05  0.06  0.10  0.14   2.41  ... -0.03  0.02   \n",
       "4   0  80  0.07  0.03  0.04  0.04  0.04  0.06  0.06   5.55  ... -0.02  0.01   \n",
       "5   0  81  0.09  0.02  0.03  0.04  0.06  0.08  0.11   2.85  ...  0.02  0.07   \n",
       "\n",
       "     R17   R18    R19   R20   R21   R22   R23   R24  \n",
       "NO                                                   \n",
       "1   0.15  0.23   3.56  0.26  1.55  0.43  0.11  0.17  \n",
       "2   0.16  0.22   3.78  1.29  1.40  0.06  0.07  0.10  \n",
       "3   0.02  0.04  13.29  1.61  1.43  0.03  0.05  0.07  \n",
       "4   0.02  0.02   5.36  1.30  1.12 -0.06 -0.08 -0.09  \n",
       "5   0.10  0.14   7.74  1.48  1.41  0.03  0.04  0.06  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankruptcy_df = pd.read_csv('../data/Bankruptcy.csv',index_col='NO')\n",
    "bankruptcy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D</th>\n",
       "      <th>YR</th>\n",
       "      <th>R1</th>\n",
       "      <th>R2</th>\n",
       "      <th>R3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NO</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    D  YR    R1    R2    R3\n",
       "NO                         \n",
       "1   0  78  0.23  0.08  0.02\n",
       "2   0  77  0.19  0.07  0.09\n",
       "3   0  72  0.07  0.02  0.03\n",
       "4   0  80  0.07  0.03  0.04\n",
       "5   0  81  0.09  0.02  0.03"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Also, we can limit the columns we can read so instead of all 26 columns - let's say I'm only interested in the D, YR, and R1-R3 (notice, since I want NO as the index, it needs to be in the column list)\n",
    "bankruptcy_df = pd.read_csv('../data/Bankruptcy.csv',index_col='NO',usecols=['NO','D','YR','R1','R2','R3'])\n",
    "bankruptcy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputing using pandas\n",
    "It is also helpful to gather data in one format and write to another.  There are many, many ways to go about this, but one simple way to handle this is to use the `to_*` functions.  We'll look at a few others, but for now, let's say we have cleaned up our bankruptcy file and need to import into Excel.  We can easily write it to a csv file with a simple command.  Run the next cell and then check out the result.\n",
    "\n",
    "{download}`my_new_file.csv <../data/my_new_file.csv>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output to a CSV file\n",
    "bankruptcy_df.to_csv('../output/my_new_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured text formats \n",
    "XML (and JSON) provide similar portability as you can find with CSV files, but they tend to be more descriptive.  These formats allow for sub-records and descriptive field names.  They handle missing data a bit more effectively and obviously especially if they are being read by a human.  The second most common way to get data from the internet is through web-scraping or an API (application programming interface). Often, APIs will return a more structured format because of the level of detail they offer.  In this case, XML and JSON are used often.  In the case of web-scraping, that is getting data from a web page that doesn't offer a programmable end-point, parsing HTML becomes an important skill. Best of all, HTML is very similar to XML (it could be called a subset), so learning to deal with XML will go along way to learning how to deal with HTML.\n",
    "\n",
    "### Extensible Mark-up Language (XML)\n",
    "While the formatting of XML files and how they work is left to the lecture (it's better described in PowerPoint rather than code), we'll look at an example here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape</th>\n",
       "      <th>degrees</th>\n",
       "      <th>sides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>square</td>\n",
       "      <td>360</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>circle</td>\n",
       "      <td>360</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>triangle</td>\n",
       "      <td>180</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      shape  degrees  sides\n",
       "0    square      360    4.0\n",
       "1    circle      360    NaN\n",
       "2  triangle      180    3.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We could just as easily read this from a file, but it helps to see the actual XML, so we'll read it directly from a string instead\n",
    "\n",
    "xml_data = '''<?xml version='1.0' encoding='utf-8'?>\n",
    "<data xmlns=\"http://example.com\">\n",
    " <row>\n",
    "   <shape>square</shape>\n",
    "   <degrees>360</degrees>\n",
    "   <sides>4.0</sides>\n",
    " </row>\n",
    " <row>\n",
    "   <shape>circle</shape>\n",
    "   <degrees>360</degrees>\n",
    "   <sides/>\n",
    " </row>\n",
    " <row>\n",
    "   <shape>triangle</shape>\n",
    "   <degrees>180</degrees>\n",
    "   <sides>3.0</sides>\n",
    " </row>\n",
    "</data>'''\n",
    "df = pd.read_xml(xml_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical data\n",
    "While this works fine with data that is just one level deep (for instance, we just have `rows` in the prior data.)  Things get a bit more complex and unwieldy to use `pandas` for complex data types.  Say for instance we want details about a cd collection\n",
    "\n",
    "```xml\n",
    "<music>\n",
    "  <artist name=\"Radiohead\">\n",
    "    <album title=\"The King of Limbs\">\n",
    "      <song title=\"Bloom\" length=\"5:15\"/>\n",
    "      <song title=\"Morning Mr Magpie\" length=\"4:41\"/>\n",
    "      <song title=\"Little by Little\" length=\"4:27\"/>\n",
    "      <song title=\"Feral\" length=\"3:13\"/>\n",
    "      <song title=\"Lotus Flower\" length=\"5:01\"/>\n",
    "      <song title=\"Codex\" length=\"4:47\"/>\n",
    "      <song title=\"Give Up the Ghost\" length=\"4:50\"/>\n",
    "      <song title=\"Separator\" length=\"5:20\"/>\n",
    "      <description link=\"http://en.wikipedia.org/wiki/The_King_of_Limbs\">\n",
    "\tThe King of Limbs is the eighth studio album by English rock band Radiohead, produced by Nigel Godrich. It was self-released on 18 February 2011 as a download in MP3 and WAV formats, followed by physical CD and 12\" vinyl releases on 28 March, a wider digital release via AWAL, and a special \"newspaper\" edition on 9 May 2011. The physical editions were released through the band's Ticker Tape imprint on XL in the United Kingdom, TBD in the United States, and Hostess Entertainment in Japan.\n",
    "      </description>\n",
    "    </album>\n",
    "    <album title=\"OK Computer\">\n",
    "      <song title=\"Airbag\"  length=\"4:44\"/>\n",
    "      <song title=\"Paranoid Android\"  length=\"6:23\"/>\n",
    "      <song title=\"Subterranean Homesick Alien\"  length=\"4:27\"/>\n",
    "      <song title=\"Exit Music (For a Film)\"  length=\"4:24\"/>\n",
    "      <song title=\"Let Down\"  length=\"4:59\"/>\n",
    "      <song title=\"Karma Police\"  length=\"4:21\"/>\n",
    "      <song title=\"Fitter Happier\"  length=\"1:57\"/>\n",
    "      <song title=\"Electioneering\"  length=\"3:50\"/>\n",
    "      <song title=\"Climbing Up the Walls\"  length=\"4:45\"/>\n",
    "      <song title=\"No Surprises\"  length=\"3:48\"/>\n",
    "      <song title=\"Lucky\"  length=\"4:19\"/>\n",
    "      <song title=\"The Tourist\"  length=\"5:24\"/>\n",
    "      <description link=\"http://en.wikipedia.org/wiki/OK_Computer\">\n",
    "\tOK Computer is the third studio album by the English alternative rock band Radiohead, released on 16 June 1997 on Parlophone in the United Kingdom and 1 July 1997 by Capitol Records in the United States. It marks a deliberate attempt by the band to move away from the introspective guitar-oriented sound of their previous album The Bends. Its layered sound and wide range of influences set it apart from many of the Britpop and alternative rock bands popular at the time and laid the groundwork for Radiohead's later, more experimental work.\n",
    "      </description>\n",
    "    </album>\n",
    "  </artist>\n",
    "  <artist name=\"Portishead\">\n",
    "    <album title=\"Dummy\">\n",
    "      <song title=\"Mysterons\"  length=\"5:02\"/>\n",
    "      <song title=\"Sour Times\"  length=\"4:11\"/>\n",
    "      <song title=\"Strangers\"  length=\"3:55\"/>\n",
    "      <song title=\"It Could Be Sweet\"  length=\"4:16\"/>\n",
    "      <song title=\"Wandering Star\"  length=\"4:51\"/>\n",
    "      <song title=\"It's a Fire\"  length=\"3:49\"/>\n",
    "      <song title=\"Numb\"  length=\"3:54\"/>\n",
    "      <song title=\"Roads\"  length=\"5:02\"/>\n",
    "      <song title=\"Pedestal\"  length=\"3:39\"/>\n",
    "      <song title=\"Biscuit\"  length=\"5:01\"/>\n",
    "      <song title=\"Glory Box\"  length=\"5:06\"/>\n",
    "      <description link=\"http://en.wikipedia.org/wiki/Dummy_%28album%29\">\n",
    "\tDummy is the debut album of the Bristol-based group Portishead. Released in August 22, 1994 on Go! Discs, the album earned critical acclaim, winning the 1995 Mercury Music Prize. It is often credited with popularizing the trip-hop genre and is frequently cited in lists of the best albums of the 1990s. Although it achieved modest chart success overseas, it peaked at #2 on the UK Album Chart and saw two of its three singles reach #13. The album was certified gold in 1997 and has sold two million copies in Europe. As of September 2011, the album was certified double-platinum in the United Kingdom and has sold as of September 2011 825,000 copies.\n",
    "      </description>\n",
    "    </album>\n",
    "  </artist>\n",
    "</music>\n",
    "```\n",
    "This doesn't look like the tabular data we are use to, each item in the list is an album, but each album can also include some songs.  In this case, we need to depend on other libraries to help us out.\n",
    "\n",
    "We first need to read in the tree, then we can deal with each of the records individually.  Keep in mind we have attributes and tags here so there's a need to be a bit more specific.  And we'll need to use a bit of XPath to get there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Artists\n",
      "{'name': 'Radiohead'}\n",
      "{'name': 'Portishead'}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "xml_data = '''\n",
    "<music>\n",
    "  <artist name=\"Radiohead\">\n",
    "    <album title=\"The King of Limbs\">\n",
    "      <song title=\"Bloom\" length=\"5:15\"/>\n",
    "      <song title=\"Morning Mr Magpie\" length=\"4:41\"/>\n",
    "      <song title=\"Little by Little\" length=\"4:27\"/>\n",
    "      <song title=\"Feral\" length=\"3:13\"/>\n",
    "      <song title=\"Lotus Flower\" length=\"5:01\"/>\n",
    "      <song title=\"Codex\" length=\"4:47\"/>\n",
    "      <song title=\"Give Up the Ghost\" length=\"4:50\"/>\n",
    "      <song title=\"Separator\" length=\"5:20\"/>\n",
    "      <description link=\"http://en.wikipedia.org/wiki/The_King_of_Limbs\">\n",
    "\tThe King of Limbs is the eighth studio album by English rock band Radiohead, produced by Nigel Godrich. It was self-released on 18 February 2011 as a download in MP3 and WAV formats, followed by physical CD and 12\" vinyl releases on 28 March, a wider digital release via AWAL, and a special \"newspaper\" edition on 9 May 2011. The physical editions were released through the band's Ticker Tape imprint on XL in the United Kingdom, TBD in the United States, and Hostess Entertainment in Japan.\n",
    "      </description>\n",
    "    </album>\n",
    "    <album title=\"OK Computer\">\n",
    "      <song title=\"Airbag\"  length=\"4:44\"/>\n",
    "      <song title=\"Paranoid Android\"  length=\"6:23\"/>\n",
    "      <song title=\"Subterranean Homesick Alien\"  length=\"4:27\"/>\n",
    "      <song title=\"Exit Music (For a Film)\"  length=\"4:24\"/>\n",
    "      <song title=\"Let Down\"  length=\"4:59\"/>\n",
    "      <song title=\"Karma Police\"  length=\"4:21\"/>\n",
    "      <song title=\"Fitter Happier\"  length=\"1:57\"/>\n",
    "      <song title=\"Electioneering\"  length=\"3:50\"/>\n",
    "      <song title=\"Climbing Up the Walls\"  length=\"4:45\"/>\n",
    "      <song title=\"No Surprises\"  length=\"3:48\"/>\n",
    "      <song title=\"Lucky\"  length=\"4:19\"/>\n",
    "      <song title=\"The Tourist\"  length=\"5:24\"/>\n",
    "      <description link=\"http://en.wikipedia.org/wiki/OK_Computer\">\n",
    "\tOK Computer is the third studio album by the English alternative rock band Radiohead, released on 16 June 1997 on Parlophone in the United Kingdom and 1 July 1997 by Capitol Records in the United States. It marks a deliberate attempt by the band to move away from the introspective guitar-oriented sound of their previous album The Bends. Its layered sound and wide range of influences set it apart from many of the Britpop and alternative rock bands popular at the time and laid the groundwork for Radiohead's later, more experimental work.\n",
    "      </description>\n",
    "    </album>\n",
    "  </artist>\n",
    "  <artist name=\"Portishead\">\n",
    "    <album title=\"Dummy\">\n",
    "      <song title=\"Mysterons\"  length=\"5:02\"/>\n",
    "      <song title=\"Sour Times\"  length=\"4:11\"/>\n",
    "      <song title=\"Strangers\"  length=\"3:55\"/>\n",
    "      <song title=\"It Could Be Sweet\"  length=\"4:16\"/>\n",
    "      <song title=\"Wandering Star\"  length=\"4:51\"/>\n",
    "      <song title=\"It's a Fire\"  length=\"3:49\"/>\n",
    "      <song title=\"Numb\"  length=\"3:54\"/>\n",
    "      <song title=\"Roads\"  length=\"5:02\"/>\n",
    "      <song title=\"Pedestal\"  length=\"3:39\"/>\n",
    "      <song title=\"Biscuit\"  length=\"5:01\"/>\n",
    "      <song title=\"Glory Box\"  length=\"5:06\"/>\n",
    "      <description link=\"http://en.wikipedia.org/wiki/Dummy_%28album%29\">\n",
    "\tDummy is the debut album of the Bristol-based group Portishead. Released in August 22, 1994 on Go! Discs, the album earned critical acclaim, winning the 1995 Mercury Music Prize. It is often credited with popularizing the trip-hop genre and is frequently cited in lists of the best albums of the 1990s. Although it achieved modest chart success overseas, it peaked at #2 on the UK Album Chart and saw two of its three singles reach #13. The album was certified gold in 1997 and has sold two million copies in Europe. As of September 2011, the album was certified double-platinum in the United Kingdom and has sold as of September 2011 825,000 copies.\n",
    "      </description>\n",
    "    </album>\n",
    "    <album title=\"Third\">\n",
    "      <song title=\"Silence\"  length=\"4:58\"/>\n",
    "      <song title=\"Hunter\"  length=\"3:57\"/>\n",
    "      <song title=\"Nylon Smile\"  length=\"3:16\"/>\n",
    "      <song title=\"The Rip\"  length=\"4:29\"/>\n",
    "      <song title=\"Plastic\"  length=\"3:27\"/>\n",
    "      <song title=\"We Carry On\"  length=\"6:27\"/>\n",
    "      <song title=\"Deep Water\"  length=\"1:31\"/>\n",
    "      <song title=\"Machine Gun\"  length=\"4:43\"/>\n",
    "      <song title=\"Small\"  length=\"6:45\"/>\n",
    "      <song title=\"Magic Doors\"  length=\"3:32\"/>\n",
    "      <song title=\"Threads\"  length=\"5:45\"/>\n",
    "      <description link=\"http://en.wikipedia.org/wiki/Third_%28Portishead_album%29\">\n",
    "\tThird is the third studio album by English musical group Portishead, released on 27 April 2008, on Island Records in the United Kingdom, two days after on Mercury Records in the United States, and on 30 April 2008 on Universal Music Japan in Japan. It is their first release in 10 years, and their first studio album in eleven years. Third entered the UK Album Chart at #2, and became the band's first-ever American Top 10 album on the Billboard 200, reaching #7 in its entry week.\n",
    "      </description>\n",
    "    </album>\n",
    "  </artist>\n",
    "</music>\n",
    "'''\n",
    "\n",
    "tree = ET.fromstring(xml_data)\n",
    "print('All Artists')\n",
    "for artist in tree.findall('artist'):\n",
    "    print(artist.attrib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Artists/Albums\n",
      "Artist: Radiohead\n",
      "\tThe King of Limbs\n",
      "\tOK Computer\n",
      "Artist: Portishead\n",
      "\tDummy\n",
      "\tThird\n"
     ]
    }
   ],
   "source": [
    "# Interesting, but what about albums, can we find all the albums for each artist?\n",
    "# Notice we can also read the data from a file rather than a string\n",
    "tree = ET.parse('../data/music.xml')\n",
    "print('All Artists/Albums')\n",
    "for artist in tree.findall('artist'):\n",
    "    # This time, just get the value of the 'name' attribute\n",
    "    print('Artist:', artist.attrib['name'])\n",
    "    # Notice we are searching starting with the current artist to find all their albums\n",
    "    for album in artist.findall('album'):\n",
    "        # Again, since we know the attribute we are after we'll just index it directly\n",
    "        print(f'\\t{album.attrib[\"title\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Artists/Albums\n",
      "Artist: Radiohead\n",
      "\tAlbum: The King of Limbs\n",
      "\t\tBloom\n",
      "\t\tMorning Mr Magpie\n",
      "\t\tLittle by Little\n",
      "\t\tFeral\n",
      "\t\tLotus Flower\n",
      "\t\tCodex\n",
      "\t\tGive Up the Ghost\n",
      "\t\tSeparator\n",
      "\tAlbum: OK Computer\n",
      "\t\tAirbag\n",
      "\t\tParanoid Android\n",
      "\t\tSubterranean Homesick Alien\n",
      "\t\tExit Music (For a Film)\n",
      "\t\tLet Down\n",
      "\t\tKarma Police\n",
      "\t\tFitter Happier\n",
      "\t\tElectioneering\n",
      "\t\tClimbing Up the Walls\n",
      "\t\tNo Surprises\n",
      "\t\tLucky\n",
      "\t\tThe Tourist\n",
      "Artist: Portishead\n",
      "\tAlbum: Dummy\n",
      "\t\tMysterons\n",
      "\t\tSour Times\n",
      "\t\tStrangers\n",
      "\t\tIt Could Be Sweet\n",
      "\t\tWandering Star\n",
      "\t\tIt's a Fire\n",
      "\t\tNumb\n",
      "\t\tRoads\n",
      "\t\tPedestal\n",
      "\t\tBiscuit\n",
      "\t\tGlory Box\n",
      "\tAlbum: Third\n",
      "\t\tSilence\n",
      "\t\tHunter\n",
      "\t\tNylon Smile\n",
      "\t\tThe Rip\n",
      "\t\tPlastic\n",
      "\t\tWe Carry On\n",
      "\t\tDeep Water\n",
      "\t\tMachine Gun\n",
      "\t\tSmall\n",
      "\t\tMagic Doors\n",
      "\t\tThreads\n"
     ]
    }
   ],
   "source": [
    "# Now let's find all the songs on each album\n",
    "tree = ET.parse('../data/music.xml')\n",
    "print('All Artists/Albums')\n",
    "for artist in tree.findall('artist'):\n",
    "    # Notice, we can use the index or directly use `get`\n",
    "    print('Artist:', artist.get('name'))\n",
    "    # Notice we are searching starting with the current artist to find all their albums\n",
    "    for album in artist.findall('album'):\n",
    "        # Again, since we know the attribute we are after we'll just index it directly\n",
    "        print(f'\\tAlbum: {album.attrib[\"title\"]}')\n",
    "        for song in album.findall('song'):\n",
    "            print(f'\\t\\t{song.get(\"title\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further with XPath support\n",
    "This is the simplest situation, where we are just wanting to iterate through every element in the XML tree.  We can get much more creative with a specific query syntax for XML called [XPath](https://docs.python.org/3/library/xml.etree.elementtree.html#elementtree-xpath).  XPath allows for alot more complex situations, such as if we want to find only songs on albums by Radiohead or gathering the album description links only.  See these examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radiohead only\n",
      "Artist: Radiohead\n",
      "\t\tBloom\n",
      "\t\tMorning Mr Magpie\n",
      "\t\tLittle by Little\n",
      "\t\tFeral\n",
      "\t\tLotus Flower\n",
      "\t\tCodex\n",
      "\t\tGive Up the Ghost\n",
      "\t\tSeparator\n",
      "\t\tAirbag\n",
      "\t\tParanoid Android\n",
      "\t\tSubterranean Homesick Alien\n",
      "\t\tExit Music (For a Film)\n",
      "\t\tLet Down\n",
      "\t\tKarma Police\n",
      "\t\tFitter Happier\n",
      "\t\tElectioneering\n",
      "\t\tClimbing Up the Walls\n",
      "\t\tNo Surprises\n",
      "\t\tLucky\n",
      "\t\tThe Tourist\n"
     ]
    }
   ],
   "source": [
    "# Only songs off of Radiohead albums\n",
    "tree = ET.fromstring(xml_data)\n",
    "print('Radiohead only')\n",
    "for artist in tree.findall('./artist/[@name=\"Radiohead\"]'):\n",
    "    print('Artist:', artist.get('name'))\n",
    "    # Notice we are searching starting with the current artist to find all their albums\n",
    "    for song in artist.findall('.//album/song'):\n",
    "        print(f'\\t\\t{song.get(\"title\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only album name and description links\n",
      "Album: The King of Limbs\n",
      "Link: http://en.wikipedia.org/wiki/The_King_of_Limbs\n",
      "Album: OK Computer\n",
      "Link: http://en.wikipedia.org/wiki/OK_Computer\n",
      "Album: Dummy\n",
      "Link: http://en.wikipedia.org/wiki/Dummy_%28album%29\n",
      "Album: Third\n",
      "Link: http://en.wikipedia.org/wiki/Third_%28Portishead_album%29\n"
     ]
    }
   ],
   "source": [
    "# Only album names and descriptions\n",
    "tree = ET.fromstring(xml_data)\n",
    "print('Only album name and description links')\n",
    "for album in tree.findall('.//album'):\n",
    "    print('Album:', album.get('title'))\n",
    "    print('Link:',album.find('description').get('link'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Javascript Notation\n",
    "JSON is similar, a little less decoration (no `<>` and `</>`) but still very readable.  In the case of Python, JSON data is parsed into a dictionary with one key for every top-level element and the corresponding structure as it's value.  For instance, if we have an object like such, then we have one key in the dictionary and the value is a list.\n",
    "\n",
    "```json\n",
    "    \"shapes\":\n",
    "    [\n",
    "        {\"shape\": \"square\",\"degrees\": 360,\"sides\": 4.0},\n",
    "        {\"shape\": \"circle\",\"degrees\": 360},\n",
    "        {\"shape\": \"triangle\",\"degrees\": 180,\"sides\": 3.0}\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shapes': [{'shape': 'square', 'degrees': 360, 'sides': 4.0}, {'shape': 'circle', 'degrees': 360}, {'shape': 'triangle', 'degrees': 180, 'sides': 3.0}]}\n",
      "[{'shape': 'square', 'degrees': 360, 'sides': 4.0}, {'shape': 'circle', 'degrees': 360}, {'shape': 'triangle', 'degrees': 180, 'sides': 3.0}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Same data as above, described in JSON format\n",
    "json_data = '''{\"shapes\":\n",
    "    [\n",
    "        {\"shape\": \"square\",\"degrees\": 360,\"sides\": 4.0},\n",
    "        {\"shape\": \"circle\",\"degrees\": 360},\n",
    "        {\"shape\": \"triangle\",\"degrees\": 180,\"sides\": 3.0}\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "# Notice the use of `loads` this is to load json from a string\n",
    "data = json.loads(json_data)\n",
    "print(data)\n",
    "print(data['shapes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shape': 'square', 'degrees': 360, 'sides': 4.0}\n",
      "{'shape': 'circle', 'degrees': 360}\n",
      "{'shape': 'triangle', 'degrees': 180, 'sides': 3.0}\n"
     ]
    }
   ],
   "source": [
    "for row in data['shapes']:\n",
    "    # Notice that the items in the list are themselves key/value pairs or as we know them dictionaries\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the values themselves are dictionaries, we can access the values with string indicies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: square\n",
      "Degrees: 360\n",
      "Sides: 4.0\n",
      "\n",
      "Shape: circle\n",
      "Degrees: 360\n",
      "Sides: 0\n",
      "\n",
      "Shape: triangle\n",
      "Degrees: 180\n",
      "Sides: 3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Since the values are dictionaries, we can access the values with string indexes\n",
    "for row in data['shapes']:\n",
    "    # Notice that the items in the list are themselves key/value pairs or as we know them dictionaries\n",
    "    print(f\"Shape: {row['shape']}\")\n",
    "    print(f\"Degrees: {row['degrees']}\")\n",
    "    # But keep in mind, that we aren't guaranteed to have all the same name/value pairs\n",
    "    #  For instance, circles don't have a 'sides' key if we don't use get(), we'll end up with an error\n",
    "    # So using the 'get' method is a better option when dealing with unreliable data\n",
    "    print(f\"Sides: {row.get('sides',0)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{note}\n",
    "The example aboves uses the built-in dictionary function `get()`.  It is helpful to use `get()` when we aren't positive or guaranteed to get have a key where we expect it to be.  For instance, in the dictionary provided above, circles don't have sides, so this value is left out of the reponse.  So if we were to try and access a key called sides, we'd get an error reminding us that there was no such key.  By using `get()` instead, we've told Python - \"if you don't have a key with the value we expect, then just give us back this other default value (in our case 0).\"\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shapes': [{'shape': 'square', 'degrees': 360, 'sides': 4.0}, {'shape': 'circle', 'degrees': 360}, {'shape': 'triangle', 'degrees': 180, 'sides': 3.0}]}\n"
     ]
    }
   ],
   "source": [
    "# We can also load the data directly from a file\n",
    "f = open('../data/shapes.json')\n",
    "json_data_file = json.load(f)\n",
    "print(json_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just remember that with JSON we are dealing with dictionaries and lists, so accessing the items is the same thing we know how to do with them.  The complexity comes in keeping track of when we have a dictionary and when we have a list - and then working with the items appropriately.\n",
    "```json\n",
    "{\n",
    "  \"data\": [{\n",
    "    \"type\": \"articles\",\n",
    "    \"id\": \"1\",\n",
    "    \"attributes\": {\n",
    "      \"title\": \"JSON:API paints my bikeshed!\",\n",
    "      \"body\": \"The shortest article. Ever.\",\n",
    "      \"created\": \"2015-05-22T14:56:29.000Z\",\n",
    "      \"updated\": \"2015-05-22T14:56:28.000Z\"\n",
    "    },\n",
    "    \"relationships\": {\n",
    "      \"author\": {\n",
    "        \"data\": {\"id\": \"42\", \"type\": \"people\"}\n",
    "      }\n",
    "    }\n",
    "  }],\n",
    "  \"included\": [\n",
    "    {\n",
    "      \"type\": \"people\",\n",
    "      \"id\": \"42\",\n",
    "      \"attributes\": {\n",
    "        \"name\": \"John\",\n",
    "        \"age\": 80,\n",
    "        \"gender\": \"male\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "In this example, we have a dictionary with two \"keys\" - `data` and `included`.  These keys have associated values which are lists (see the `[]`) even though their is only one item in each of these value lists.  So in order to process the \"sub-dictionaries\" it becomes necessary first to get the first item in the list, then use the resulting dictionary to parse the next set of name/value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "included\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "json_data = \"\"\"{\n",
    "  \"data\": [{\n",
    "    \"type\": \"articles\",\n",
    "    \"id\": \"1\",\n",
    "    \"attributes\": {\n",
    "      \"title\": \"JSON:API paints my bikeshed!\",\n",
    "      \"body\": \"The shortest article. Ever.\",\n",
    "      \"created\": \"2015-05-22T14:56:29.000Z\",\n",
    "      \"updated\": \"2015-05-22T14:56:28.000Z\"\n",
    "    },\n",
    "    \"relationships\": {\n",
    "      \"author\": {\n",
    "        \"data\": {\"id\": \"42\", \"type\": \"people\"}\n",
    "      }\n",
    "    }\n",
    "  }],\n",
    "  \"included\": [\n",
    "    {\n",
    "      \"type\": \"people\",\n",
    "      \"id\": \"42\",\n",
    "      \"attributes\": {\n",
    "        \"name\": \"John\",\n",
    "        \"age\": 80,\n",
    "        \"gender\": \"male\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "api_data = json.loads(json_data)\n",
    "# Notice only two keys in the json data\n",
    "for k in api_data.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'articles', 'id': '1', 'attributes': {'title': 'JSON:API paints my bikeshed!', 'body': 'The shortest article. Ever.', 'created': '2015-05-22T14:56:29.000Z', 'updated': '2015-05-22T14:56:28.000Z'}, 'relationships': {'author': {'data': {'id': '42', 'type': 'people'}}}}]\n"
     ]
    }
   ],
   "source": [
    "# This is a list with one item, a dictionary\n",
    "print(api_data['data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'articles', 'id': '1', 'attributes': {'title': 'JSON:API paints my bikeshed!', 'body': 'The shortest article. Ever.', 'created': '2015-05-22T14:56:29.000Z', 'updated': '2015-05-22T14:56:28.000Z'}, 'relationships': {'author': {'data': {'id': '42', 'type': 'people'}}}}\n"
     ]
    }
   ],
   "source": [
    "# So now we get the first item in the list, and it's a dictionary\n",
    "data_dictionary = api_data['data'][0]\n",
    "print(data_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell may be a bit confusing because we haven't covered it before.  When we iterate (loop through) a dictionary, Python returns two values rather than one like in a typically loop.  In this case, it returns both a key and the associated value when we ask for the items.  Therefore, `dd_key` will hold the current key and `dd_value` will hold the value associated with the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: articles\n",
      "id: 1\n",
      "attributes: {'title': 'JSON:API paints my bikeshed!', 'body': 'The shortest article. Ever.', 'created': '2015-05-22T14:56:29.000Z', 'updated': '2015-05-22T14:56:28.000Z'}\n",
      "relationships: {'author': {'data': {'id': '42', 'type': 'people'}}}\n"
     ]
    }
   ],
   "source": [
    "# With the `data_dictionary` in hand, we can access the items\n",
    "for dd_key, dd_value in data_dictionary.items():\n",
    "    print(f'{dd_key}: {dd_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'JSON:API paints my bikeshed!', 'body': 'The shortest article. Ever.', 'created': '2015-05-22T14:56:29.000Z', 'updated': '2015-05-22T14:56:28.000Z'}\n",
      "\n",
      "title: JSON:API paints my bikeshed!\n",
      "body: The shortest article. Ever.\n",
      "created: 2015-05-22T14:56:29.000Z\n",
      "updated: 2015-05-22T14:56:28.000Z\n"
     ]
    }
   ],
   "source": [
    "# We notice that again we have two more 'sub-dictionaries', attributes and relationships.  \n",
    "attributes_dict = data_dictionary['attributes']\n",
    "print(attributes_dict)\n",
    "print()\n",
    "for a_key, a_value in attributes_dict.items():\n",
    "    print(f'{a_key}: {a_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': {'data': {'id': '42', 'type': 'people'}}}\n",
      "\n",
      "{'data': {'id': '42', 'type': 'people'}}\n",
      "\n",
      "{'id': '42', 'type': 'people'}\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "# Relationships is even more complex with extra sub-dictionaries, so take it one step at a time\n",
    "relationships_dict = data_dictionary['relationships']\n",
    "print(relationships_dict)\n",
    "print()\n",
    "author_dict=relationships_dict['author']\n",
    "print(author_dict)\n",
    "print()\n",
    "data_dict = author_dict['data']\n",
    "print(data_dict)\n",
    "print(data_dict[\"id\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
